{
  "pipe-parallel-size": 1,
  "model-parallel-size": 1,

  # model settings
  "num-layers": 24,
  "hidden-size": 1920,
  "num-attention-heads": 15,
  "seq-length": 1024,
  "max-position-embeddings": 1024,
  "norm": "layernorm",
  "pos-emb": "learned",
  "scaled-upper-triang-masked-softmax-fusion": true,
  "bias-gelu-fusion": true,

  # optimizer settings
  "optimizer":
    { "type": "Adam", "params": { "lr": 0.0002, "betas": [0.9, 0.95] } },

  # batch / data settings
  "train_micro_batch_size_per_gpu": 16,
  "gradient_accumulation_steps": 1,
  "data-impl": "mmap",
  "split": "949,50,1",

  # activation checkpointing
  "checkpoint-activations": true,
  "checkpoint-num-layers": 6,
  "partition-activations": false,
  "synchronize-each-layer": false,

  # regularization
  "gradient_clipping": 1.0,
  "weight-decay": 0,
  "hidden-dropout": 0,
  "attention-dropout": 0,

  # precision settings
  "fp16":
    {
      "fp16": true,
      "enabled": true,
      "loss_scale": 0,
      "loss_scale_window": 1000,
      "initial_scale_power": 12,
      "hysteresis": 2,
      "min_loss_scale": 1,
    },

  # misc. training settings
  "train-iters": 50,
  "lr-decay-iters": 320000,
  "distributed-backend": "nccl",
  "lr-decay-style": "cosine",
  "warmup": 0.01,
  "save-interval": 10000,
  "eval-interval": 1000,
  "eval-iters": 0,

  # logging
  "log-interval": 5,
  "steps_per_print": 5,
  "keep-last-n-checkpoints": 4,
  "wall_clock_breakdown": true,
}
